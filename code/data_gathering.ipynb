{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dbff2901-c801-4a4d-9130-424a9afff8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import package\n",
    "import requests \n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# please uncomment code below this comment for install tqdm package if you don't have\n",
    "# !conda install -c conda-forge tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ba936e-d405-4255-b2bc-905d8d440bbd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c915ae-41ba-4f4b-a91b-a364f5aeb4d4",
   "metadata": {},
   "source": [
    "## Data Gathering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bed28f5-8030-45e3-85a4-24c0de27dcf1",
   "metadata": {},
   "source": [
    "Data from reddit used api for collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d019015-007d-4c25-bf62-80e138091856",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create function for gathering data from reddit\n",
    "def get_subreddit(subreddit, size):\n",
    "    '''\n",
    "    parameter\n",
    "    ----------\n",
    "    subreddit: subreddit from reddit need to assign in string type\n",
    "    size: quantity of threds (if size more than threads in subreddit this function\n",
    "          will generate duplicate values)\n",
    "    \n",
    "    return\n",
    "    ----------\n",
    "    threads from subreddit in dataframe form\n",
    "    '''\n",
    "    headers = {'User-Agent': 'Test'}\n",
    "    posts = []\n",
    "    after = None\n",
    "    for _ in tqdm(range(int(size/25))):\n",
    "        if after == None:\n",
    "            params = {}\n",
    "        else:\n",
    "            params = {'after': after}\n",
    "        url = f'https://www.reddit.com/r/{subreddit}.json'\n",
    "        res = requests.get(url, params = params, headers=headers)\n",
    "        if res.status_code == 200:\n",
    "            the_json = res.json()\n",
    "            posts.extend(the_json['data']['children'])\n",
    "            after = the_json['data']['after']\n",
    "            # backup data if error happen\n",
    "            backup = pd.DataFrame([posts[i]['data'] for i in range(len(posts))])\n",
    "            backup.to_csv(f'../data/{subreddit}_backup.csv')\n",
    "        else:\n",
    "            print('Error')\n",
    "            print(res.status_code)\n",
    "            break\n",
    "        \n",
    "        # increase time for not trigger the reddit server\n",
    "        time.sleep(np.random.randint(1, 2))\n",
    "        \n",
    "    # change list of data to dataframe\n",
    "    df_subreddit = pd.DataFrame([posts[i]['data'] for i in range(len(posts))])\n",
    "    print(f'{subreddit} corpus has {df_subreddit.shape[0]} documents and has {df_subreddit.shape[1]} features.')\n",
    "          \n",
    "    return df_subreddit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95980188-984b-49be-8f6a-1a995ea4c457",
   "metadata": {},
   "source": [
    "### Civil Engineering Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "337a1b31-976f-414d-9770-d4ad2c24d8c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [02:05<00:00,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "civilengineering corpus has 997 documents and has 113 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# pull threads in spacex subreddit \n",
    "civil_corpus = get_subreddit('civilengineering', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e9524603-3b34-43b2-9bce-fb219130402c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check duplicated rows in renew corpus\n",
    "civil_corpus['title'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "93de7ec4-63d7-4450-8df4-d1ea53e1d004",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicated rows in renew corpus\n",
    "civil_corpus.drop_duplicates(subset='title', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e261c9d8-8c4f-4b98-9933-427239f7f6c8",
   "metadata": {},
   "source": [
    "### Architecture Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "80e86e69-3a89-45b9-b83e-0a649c919227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 40/40 [01:54<00:00,  2.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "architecture corpus has 983 documents and has 118 features.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# pull threads in nasa subreddit \n",
    "arch_corpus = get_subreddit('architecture', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de184cf9-0f83-442c-8c0f-87e1dbbb07e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check duplicated rows in nasa corpus\n",
    "arch_corpus['title'].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b32af17d-733a-41a2-bea1-0166628b9b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicated rows in nasa corpus\n",
    "arch_corpus.drop_duplicates(subset='title', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493dd40d-686d-407f-b09e-59b98956b157",
   "metadata": {},
   "source": [
    "### Merge SpaceX and Nasa Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "9bab68b5-64fe-45e1-a26c-0eed70eb531d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "civil corpus 991 rows 113 columns\n",
      "arch corpus 898 rows 118 columns\n"
     ]
    }
   ],
   "source": [
    "# count rows and columns in corpus\n",
    "print(f'civil corpus {civil_corpus.shape[0]} rows {civil_corpus.shape[1]} columns')\n",
    "print(f'arch corpus {arch_corpus.shape[0]} rows {arch_corpus.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "4d0333b4-d71d-4768-9a31-79b679c9fe19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge spacex and nasa corpus\n",
    "corpus = pd.concat([civil_corpus, arch_corpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5171e43c-2d55-48af-ada3-b8dff25b303c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "civilengineering    52.46162\n",
       "architecture        47.53838\n",
       "Name: subreddit, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# count subreddit values\n",
    "corpus['subreddit'].value_counts(normalize=True).mul(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "06fcac34-636c-49ff-af5b-94f0c5575e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export corpus dataset to csv\n",
    "corpus.to_csv('../data/civil_arch_corpus.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
